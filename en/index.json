[{"content":"At the pressing request of Stichting BREIN, GEITje is no longer available as of today. All model files (the weights) and checkpoints have been removed from my HuggingFace repositories.\nGEITje was a Dutch-language large open language model with 7 billion parameters, based on Mistral 7B. It was (further) trained on 10 billion tokens of Dutch text, improving its proficiency in Dutch and its knowledge of Dutch-specific topics.\nAs stated in the README, GEITje was partially trained in late 2023 using portions of the Dutch Gigacorpus. Stichting BREIN claims that some subsets of the Gigacorpus contain copyrighted material sourced from illegal sources. For this reason, they had the entire Gigacorpus taken offline in August 2024.\nBREIN has informed me that, in their view, current laws and regulations dictate that the model GEITje must also be taken offline. Copyright experts have assured me that this issue is not as black and white as claimed. However, they also acknowledge that many legal questions regarding this matter remain unanswered in Europe. I cannot afford to engage in a lengthy and costly legal battle to resolve these issues. After all, GEITje was a non-commercial, scientific hobby project. For this reason, I am complying with BREIN’s request.\nSince GEITje’s release, scientific papers have been published using GEITje to study large language models in Dutch. I had hoped GEITje would remain available to researchers to ensure the scientific reproducibility of their studies. Unfortunately, discussions with BREIN on this matter have led nowhere.\nI am grateful for the many positive responses I have received over the past year. It has also been wonderful to see how GEITje has inspired so many people. GEITje has demonstrated that a viable alternative, originating from Dutch and Flemish efforts, can exist alongside the closed language models of foreign tech giants. GEITje is no longer alone: open Dutch-language LLMs now exist in many forms and flavors, trained on a variety of different sources.\nIn my view, the future of European AI still lies in open-source AI. Only when AI is free to use, can be studied by everyone, and is freely available to modify and share for any purpose can we truly speak of sovereign AI. The French and Spanish governments have already paved the way, training fully open-source models with public funding. The path to a truly open-source Dutch-language AI landscape is still open for us to take.\nThis post was translated from the original Dutch with the help of GPT-4o.\n","permalink":"https://goingdutch.ai/en/posts/geitje-takedown/","summary":"At the pressing request of Stichting BREIN, GEITje is no longer available as of today. All model files (the weights) and checkpoints have been removed from my HuggingFace repositories.\nGEITje was a Dutch-language large open language model with 7 billion parameters, based on Mistral 7B. It was (further) trained on 10 billion tokens of Dutch text, improving its proficiency in Dutch and its knowledge of Dutch-specific topics.\nAs stated in the README, GEITje was partially trained in late 2023 using portions of the Dutch Gigacorpus.","title":"The end of GEITje 1"},{"content":"This week I was honored to star as a guest in Alexander Klöpping\u0026rsquo;s en Wietse Hage\u0026rsquo;s podcast: Poki – de Podcast over Kunstmatige Intelligentie.\nWe had a good converstation about GEITje, about finetuning Large Language models in general and finetuning for Dutch in particular. We spoke for about half an hour, and the conversation ended practically without edits in the podcast. Including what will have become a classic now: the Bassietest.\nYou can listen to the episode here (in Dutch, of course), or you can search for \u0026ldquo;Poki\u0026rdquo; in your favorite podcast app.\n","permalink":"https://goingdutch.ai/en/posts/poki-geitje/","summary":"This week I was honored to star as a guest in Alexander Klöpping\u0026rsquo;s en Wietse Hage\u0026rsquo;s podcast: Poki – de Podcast over Kunstmatige Intelligentie.\nWe had a good converstation about GEITje, about finetuning Large Language models in general and finetuning for Dutch in particular. We spoke for about half an hour, and the conversation ended practically without edits in the podcast. Including what will have become a classic now: the Bassietest.","title":"Interview in the Poki-podcast: \"The Dutch Language Model: GEITje ft. Edwin Rijgersberg\""},{"content":"The second in a series of posts about questions I get about GEITje.\n\u0026ldquo;Why the name GEITje?\u0026rdquo;\nMuppets, Cows, and Seals The name \u0026ldquo;GEITje\u0026rdquo; had actually been in the back of my head for a long time as the name for a Dutch large language model.\nNaming in the world of language models is subject to interesting trends. In 2017, the Muppet generation of language models started with Allen Institute for AI\u0026rsquo;s ELMo, followed by Google\u0026rsquo;s breakthrough BERT. Naturally, ERNIE, Grover, and BigBIRD soon followed. They were succeeded by Facebook\u0026rsquo;s improved variant of BERT: RoBERTa.\nWhat name should you give to the Dutch variants of these language models? Wietse de Vries et al. simply chose \u0026ldquo;BERTje\u0026rdquo; for their Dutch BERT model. Simple, but effective: the diminutive \u0026ldquo;-je\u0026rdquo; is immediately recognizable as Dutch, and it adds an element of cuteness. The accompanying logo, a quintessential Dutch cow, is also a shot on target.\nPieter Delobelle et al. chose a different strategy for their Dutch variant of RoBERTa. They had their model name itself, by having the model predict the mask in \u0026lt;mask\u0026gt;BERT for themselves. This resulted in the typically Dutch name RobBERT. With a corresponding cute logo of a seal (rob) dressed as Bert from Sesame Street, of course.\nLlamas and Other Camelids After the arrival of ChatGPT at the end of 2022 (sadly without a cute logo), open-source alternatives quickly followed. Meta started with LLaMA (no logo), followed by a series of finetunes that made LLaMA a chatbot: including Stanford\u0026rsquo;s Alpaca (fancy alpaca with sunglasses) and Vicuña (abstract logo by Stable Diffusion 2.1).\nUnfortunately, the llama doesn\u0026rsquo;t have a very extensive family. After the alpaca and vicuña, you have the Guanaco, but after that, the lamini are exhausted. Names of other camelids never really caught on.\nFalcons and Distant Lands The animal theme lingered for a while. For a moment, the model du jour was Falcon, from the Technology Innovation Institute (kind Abu Dhabi\u0026rsquo;s TNO). Logo: a polygonal drawing of a falcon.\nThe next open-source model to capture a place in the zeitgeist was Mistral 7B, created by the somewhat mysterious French startup Mistral AI. Model and company are named not after an animal, but after the mistral: the powerful northern wind in eastern France. The accompanying logo has nothing to do with a wind, but it does have significant nineties WordArt vibes.\nA Large Dutch Language Model Based on Mistral So: you tell me. You\u0026rsquo;re training a Dutch language model, based on Mistral 7B. What are you going to name it?\nI considered using a variant on Mistral. After all, in the Netherlands, we also have a thing for wind. But somehow, the models Westenwind 7B and Noordwester 7B just didn\u0026rsquo;t sound right.\nSo, back to the animal theme. A goat (geit), an animal you will find on every petting farm. An animal that greedily devours everything you feed it and then starts bleating loudly. What could be more appropriate for a language model? A little goat, \u0026ldquo;geitje\u0026rdquo;, because 7 billion isn\u0026rsquo;t as big as some other models. And it fits nicely with BERTje.\nSo, GEITje it is. With a cute logo, conceived by ChatGPT and DALL·E 3.\nAnd those capital letters? Good question. If you really want to, you can probably think of a sentence where the letters G, E, I, and T appear in order. GEneratIve Transformer is the best I\u0026rsquo;ve come up with so far, but you can probably do better.\nThis post was translated from the original Dutch with the help of GPT-4.\n","permalink":"https://goingdutch.ai/en/posts/naming-geitje/","summary":"The second in a series of posts about questions I get about GEITje.\n\u0026ldquo;Why the name GEITje?\u0026rdquo;\nMuppets, Cows, and Seals The name \u0026ldquo;GEITje\u0026rdquo; had actually been in the back of my head for a long time as the name for a Dutch large language model.\nNaming in the world of language models is subject to interesting trends. In 2017, the Muppet generation of language models started with Allen Institute for AI\u0026rsquo;s ELMo, followed by Google\u0026rsquo;s breakthrough BERT.","title":"GEITje FAQs: Why the name \"GEITje\"?"},{"content":"The first in a series of posts about questions I\u0026rsquo;ve gotten about GEITje.\n\u0026ldquo;Why did you create a language model?\u0026rdquo; I have received this question several times in recent weeks. Usually immediately followed by a follow-up question: \u0026ldquo;Doesn\u0026rsquo;t ChatGPT already exist?\u0026rdquo; Not a strange question, actually. Here are my three main reasons:\n1. Because open models are needed ChatGPT performs great in Dutch. If you have an application where you want to try a LLM, definitely go for ChatGPT or one of the OpenAI APIs. They are good and cheap, and you can whip up a solution in no time. And if you have a challenging use case: you can now even fine-tune their models on your data. But there are also disadvantages.\nFirstly, there are always cases where you do not want to or are not allowed to send your data to OpenAI. Careful handling of data is important, especially with regard to the GDPR and the upcoming AI Act. Therefore, you are quickly bound to models that you can run locally on your own infrastructure, to keep your private data private.\nSecondly, OpenAI, despite the increasingly ironic company name, is not very open about their models at all. They do not tell you what sources their models are trained on, what kind of filtering they apply, or even how large their models are. The only thing you get is a black box on their servers that you can talk to, and a technical report without meaningful technical details. If you want to do research into the model, for example, to determine if there are certain biases present that are important for your application, then you\u0026rsquo;re out of luck. The only thing you can do is send text to the black box and study the text you get back, but all other more advanced options are off the table. Using an open model, you can examine it in as much detail as you want.\nFinally, open models offer an opportunity to build on each other\u0026rsquo;s work, and to give something back. This way, you collectively reach a much higher level than everyone is capable of on their own, and everyone benefits from that.\nOpen models are therefore needed, and good news: they are booming in 2023! But Dutch was unfortunately lagging behind, as the open-source world mainly focused on English, Chinese, and programming languages. A familiar story for those who have read my earlier blog post about BLOOM, although this time there were initiatives already heading in the right direction.\n2. Because we can What is needed to create a Dutch language model? And is it possible to do as a hobby project? Well, no, not if you want to build it from the ground up. Meta used 184,320 GPU hours for training Llama 2 7B, which consumed about 74,000 kWh. Such budgets are unattainable for the GPU-poor. And, suppose, you manage to conjure up free computing capacity somewhere. Then you still need to get 2,000 billion tokens of Dutch text to match the same amount of text that Meta used for LLaMA 2. And once you have trained a foundation model, you also have to create (or have someone else create) tens of thousands of conversations to train your foundation model into a real chatbot.\nBut what if you make some smart and practical choices? What if you don\u0026rsquo;t start from scratch? What if you build on existing open source models, such as LLaMA 2 or Mistral? Despite it probably not being the intention, apparently enough Dutch text slipped into their training data for the models to be able to speak a decent word of Dutch. Not enough to hold a longer coherent conversation, and the knowledge about Dutch or Flemish subjects of those models is very limited, but it is certainly better than starting with nothing. The only thing you have to give up is some transparency. Unfortunately, little is known about what data these open source models are trained on.\nIf you start with such a pre-trained model, then you don\u0026rsquo;t need such gigantic amounts of text to train your language model. GEITje is trained on 10 billion tokens of Dutch text. Up to a few hundred billion tokens, it is relatively easy to come by material. Datasets of chat conversations to train a chatbot are barely available for Dutch, but they are abundantly available in English. By using GPT-3.5 as a translator, you can convert ten thousand conversations from English to Dutch for less than 100 euros.\nThen you need to get GPUs. Yes, they are extremely expensive to buy. And they are also expensive to rent from common cloud providers like AWS or Azure. But if you go for a cloud provider that has made GPUs their specialty, then it can all be a lot cheaper. Providers like Lambda Labs and RunPod can be up to 80% cheaper. If you manage to get your hands on a GPU, that is, as they are often all occupied. More on that in a later blog post.\n3. Because it is fun and educational The third reason is perhaps the most important one. It\u0026rsquo;s just an incredibly fun and educational project!\nI have been interested in language models for a long time, and that interest has only grown in the past year with the breakthrough of the LLMs. Reading about them is educational, and applying them even more so. But nothing gives you more insight into a model than having to train it yourself.\nTo make GEITje, I had to delve into the various foundation models and their pros and cons. I had to explore the quality of datasets, and I had to make decisions about selecting data. I had to find out what the different ways of evaluating models are, and which ones are applicable to Dutch. I had to parse datasets consisting of gigantic text files and split them into separate documents. I had to write training code, which gave me more insight into the details of 🤗 Hugging Face Transformers, accelerate and Datasets. I had to write and maintain an ever-growing README. I had to experiment with different ways of training on multiple GPUs at the same time, to come up with the most cost-efficient method. For the first time, I was able to see training graphs live in the cloud at Weights \u0026amp; Biases. I modified a Gradio interface to offer a live demo of GEITje chat.\nAnd finally, I simply had to debug training code and get it working in the cloud on rented GPUs. It\u0026rsquo;s quite an experience to solve a problem while not only seeing the minutes tick away on the clock but also directly on your own credit card.\nThis post was translated from the original Dutch with the help of GPT-4.\n","permalink":"https://goingdutch.ai/en/posts/why-geitje/","summary":"The first in a series of posts about questions I\u0026rsquo;ve gotten about GEITje.\n\u0026ldquo;Why did you create a language model?\u0026rdquo; I have received this question several times in recent weeks. Usually immediately followed by a follow-up question: \u0026ldquo;Doesn\u0026rsquo;t ChatGPT already exist?\u0026rdquo; Not a strange question, actually. Here are my three main reasons:\n1. Because open models are needed ChatGPT performs great in Dutch. If you have an application where you want to try a LLM, definitely go for ChatGPT or one of the OpenAI APIs.","title":"GEITje FAQs: Why I trained GEITje"},{"content":"It has been more than two weeks since I open-sourced GEITje 7B. It was an exciting moment, especially since this is my first major open source contribution. But I am very pleased to see how enthusiastic all the reactions have been!\nGEITje is a large open Dutch language model with 7 billion parameters, based on Mistral 7B. It has been further trained on 10 billion tokens of Dutch text. This has improved its Dutch language skills and increased its knowledge of Dutch topics.\nAll kinds of people have already started using it for their applications, of which we hopefully will see the first results soon. Bram VanRoy has added it to the Open Dutch LLM Evaluation Leaderboard, and also included it in his latest paper: Language Resources for Dutch Large Language Modelling. Thanks for that!\nLinks The most important links at a glance:\nGEITje on GitHub: Extensive README about the model, and the source code of course. 🤗 Hugging Face Models for direct access to the models: GEITje 7B GEITje 7B chat GEITje 7B chat v2 Chat with GEITje 7B chat v2 in 🤗 Hugging Face Spaces (thanks to Hugging Face for the community GPU grant!) Overview on 🤗 Hugging Face Collections with all models, quantized variants, and the datasets. FAQs A (still running) series of blog posts about frequently asked questions about:\nGEITje FAQs: Why I trained GEITje GEITje FAQs: Why the name \u0026ldquo;GEITje\u0026rdquo;? ","permalink":"https://goingdutch.ai/en/posts/introducing-geitje/","summary":"It has been more than two weeks since I open-sourced GEITje 7B. It was an exciting moment, especially since this is my first major open source contribution. But I am very pleased to see how enthusiastic all the reactions have been!\nGEITje is a large open Dutch language model with 7 billion parameters, based on Mistral 7B. It has been further trained on 10 billion tokens of Dutch text. This has improved its Dutch language skills and increased its knowledge of Dutch topics.","title":"GEITje 7B: A Large Open Dutch Language Model"},{"content":"Three volunteers. A couple of weeks of work. That\u0026rsquo;s what it took to add a language to BigScience BLOOM, the open multilingual language model with no fewer than 176 billion parameters that was released mid-2022. It aimed to become an open and multilingual alternative to GPT-3. In the end, 46 languages from all over the world made it into the dataset BLOOM was trained on. Even relatively small languages like Basque and Catalan managed to be included. Dutch did not. How is that possible?\nBigScience, big dreams It all started in 2021. A group of more than 1,000 researchers united in the virtual research collective BigScience. Probably triggered by the capabilities of GPT-3 and concerned about the rise of large language models that were increasingly kept to themselves by big tech companies, they participated from May 2021 in a one-year open research workshop in the field of multilingual large language models.\nFunded by the French government and the French-American start-up Hugging Face \u0026mdash; one of the hottest companies in the field of AI \u0026mdash; they wanted to achieve two things:\ncompile a very large multilingual text dataset of high quality, later named ROOTS; and train a very large multilingual language model with it that could rival GPT-3: BLOOM. They wanted to do this as openly as possible. The model had to be downloadable by everyone, so that it could be used for applications where you can\u0026rsquo;t use closed models like GPT-3. For instance, if you have confidential data that you don\u0026rsquo;t want to send to some American tech company. Or if you want to investigate the model for possible biases before deploying it. Or if you simply want to know what data the model has and hasn\u0026rsquo;t seen during the training phase.\nTo achieve this, the goal was to involve researchers from all sorts of different fields and to examine the dataset and model from multiple perspectives:\nDuring the workshop, the participants plan to investigate the dataset and the model from all angles: bias, social impact, capabilities, limitations, ethics, potential improvements, specific domain performances, carbon impact, general AI/cognitive research landscape.\nThe whole initiative is also extensively described in three separate scientific papers: about the process, about the dataset, and about the model.\nThe BigScience working groups. Akiki, Christopher, et al. \u0026ldquo;BigScience: A case study in the social construction of a multilingual large language model.\u0026rdquo; arXiv preprint arXiv:2212.04960 (2022).\nBLOOM: open, ethical, and climate-friendly The result: BLOOM, the BigScience Large Open-science Open-access Multilingual Language Model, launched in July 2022. A large open language model with 176 billion parameters that has been trained in 46 different languages (and 13 different programming languages). It is available for everyone to download, study, and use1. Not only is the final model available, but intermediate checkpoints of the model from during the training have been shared with everyone.\nThe model was trained in 117 days on more than 3,000 GPUs of the French Jean Zay supercomputer. Cost? About 3 million euros. The French supercomputer is also the source of the claim of the model\u0026rsquo;s climate friendliness. BigScience proudly states that the required electricity was largely generated by nuclear fission. As a result, the training of the model resulted in low CO2 emissions.\nThe Jean Zay Supercomputer. © Photo Library CNRS/Cyril Frésillon\nThe focus on openness and ethics earned praise from the academic world. Researchers from Stanford University recently published a study on large language models. They mapped out which of the large language models already best meet the requirements of the draft text of the European Union\u0026rsquo;s AI Act. BLOOM scored by far the best. Radboud University also conducted a comparative study on the openness of language models (leaderboard, paper). The most open model? Once again, BLOOM.\nIn 2022, it became fashionable to consider every large language model as a foundation model, and to fine-tune such a model on chat conversations to create an interactive model similar to OpenAI\u0026rsquo;s InstructGPT. Therefore, a chat version of BLOOM was also released in early November 2022: BLOOMZ (website, paper, GitHub). Unfortunately, the buzz around it got somewhat lost in the frenzy surrounding ChatGPT, which was launched less than four weeks later.\nScores of large language models on the requirements of the AI Act. Bommasani, Rishi et al. \u0026ldquo;Do Foundation Model Providers Comply with the EU AI Act?\u0026rdquo; https://crfm.stanford.edu/2023/06/15/eu-ai-act.html (2023).\nROOTS Corpus In order to train BLOOM, a dataset first had to be assembled: the ROOTS corpus. Here too, the focus was on openness and ethics. Dataset cards were published for all the datasets included in ROOTS. The data itself has been cleaned and deduplicated. Personal private information such as phone numbers, email addresses, and social media usernames have been automatically removed as much as possible. As a result, ROOTS has grown into a dataset of 1.6 terabytes of text data in 46 natural languages, supplemented with 13 different programming languages.\nThese 46 different languages form quite an interesting mix. Obviously, \u0026ldquo;high-resource\u0026rdquo; European languages such as English, French, Spanish, and Portuguese are not missing in this largely European project. In addition, Arabic and Chinese2 are also present. Finally, a number of \u0026ldquo;low-resource\u0026rdquo; languages have been deliberately added to the dataset, including several languages from the Niger-Congo language family for which there is little written text available.\nLanguages in the ROOTS corpus. Chart from the BLOOM model card.\nNotable missing languages? First and foremost: German. Additionally, Russian, and actually all Slavic languages, as well as the Scandinavian languages. And, of course, Dutch. However, relatively small languages such as Catalan and Basque are included. How can this be?\nLanguage Selection How was it determined which languages would be included and which would not? The answer is actually quite simple, but strangely enough, it is not found in the paper that describes the ROOTS corpus. Instead, it is discussed in the paper on BLOOM itself, on pages 10 and 11.\nLanguage Choices These considerations led us to an incremental process for choosing which languages were to be included in the corpus. We started with a list of eight of the world’s largest languages by number of speakers for which we did active outreach in the early stages of the project to invite fluent speakers to join the data efforts. Then, on the recommendation of language communities (Nekoto et al., 2020) we expanded Swahili in the original selection to the category of Niger-Congo languages, and Hindi and Urdu to Indic languages (Kunchukuttan et al., 2020). Finally, we proposed that any group of 3 or more participants fluent in an additional language could add it to the supported list if they would commit to selecting sources and guiding processing choices in the language in order to avoid common issues with corpora selected through automatic language identification without specific language expertise (Caswell et al., 2022).\nScao, Teven Le, et al. \u0026ldquo;Bloom: A 176b-parameter open-access multilingual language model.\u0026rdquo; arXiv preprint arXiv:2211.05100 (2022)\nVolunteers, then. To be precise: at least three volunteers who are fluent in the language and were willing to select sources, and were prepared to ensure that the processing of these sources was done correctly. I don\u0026rsquo;t know the exact details, but I estimate it at most a few weeks of work. Those were the costs to have the Dutch language benefit from a multi-million euro investment. Apparently, there were not at least three volunteers available who wanted or could do this for Dutch.\nMissed Opportunity? Could it have gone differently? Perhaps. I only heard about the existence of BigScience when it was already too late. Presumably, this is also the case for others in the Netherlands or Belgium who would have liked to contribute. Yes, if I wanted to participate, I would have had to find time somewhere. But with a clear goal in mind and the obvious interest we have as the Netherlands, I probably would have managed. It\u0026rsquo;s not often that you can benefit from someone else\u0026rsquo;s million-euro investment with a small time commitment. I probably have spent more time in meetings discussing consortia for hypothetical future Dutch language large language models than it would have taken to add Dutch to BLOOM.\nOn the other hand: despite all openness, BLOOM has not become the language model that made all other language models obsolete. With 176 billion parameters, it is indeed very large, but BLOOM is from an earlier generation than, for example, Meta\u0026rsquo;s LLaMA (70 billion parameters), which makes use of its parameters much more efficiently. In the 🤗 Open LLM Leaderboard, a list of the best-performing large language models, BLOOM-178B is not even included. Indicative of the lack of interest from the open-source community, I guess. A smaller variant of BLOOM, BLOOM-7b1 with \u0026ldquo;only\u0026rdquo; 7 billion parameters, is present, but it ranked somewhere in the bottom half. BLOOMZ — the chat version of BLOOM — is also not found on the leaderboard.\nBLOOM-7b1 on the 🤗 Open LLM Leaderboard.\nBut what does that leaderboard actually measure? Performance in English. The large open language models for Dutch are still very much in their infancy. To my knowledge, such a leaderboard does not even exist for Dutch.3 And if such a leaderboard for open Dutch language models were to exist: a hypothetical BLOOM that had also been trained on Dutch would be at the top of the list. In the lowlands of the blind, one-eyed would be king.\nLessons What lessons can the Dutch-speaking AI community take from this in my opinion?\nTo start with: we have to participate. The large American tech companies only see the Netherlands and the Dutch language as a side issue, and who can blame them? As speakers of a small language in a big world, we have to be opportunistic. If we can ride along on an existing initiative: free up capacity and do it! Volunteers wanted? We have them ready! Not just grandiose project plans, but also simple eager hands.\nWe must prevent being left behind next time. But that alone is not enough. We must, as a country — and therefore as a government — also invest in compiling, cleaning, and publishing Dutch-language datasets. Datasets for training, datasets for creating chatbots and agents, datasets for evaluating performance and measuring bias. We must bring those datasets to everyone\u0026rsquo;s attention. Publish them everywhere companies and academics looking to train a language model are acquiring their data. So not only on data.overheid.nl and the SURF Repository, but also on Github, on Hugging Face datasets, and on r/MachineLearning. Push our language until it can\u0026rsquo;t be ignored.\nDutch as something extra on the side. Not by chance, but as a national strategy.\nAnd it doesn\u0026rsquo;t end there. As a society, we must ask ourselves why our domestic technology companies cannot currently play the same role for Dutch that big tech does for English. Where are the open models of Albert Heijn, Bol.com, Booking.com, and Just Eat Takeaway? And why can the National Growth Fund invest over 200 million euros in the AINed program, but then I find zero developed open-source datasets or models on their website?\nAnd while I\u0026rsquo;m at it: is there anyone out there even thinking about language models for Frisian?\nThis post was translated from the original Dutch with the help of GPT-4.\nStrictly speaking, the model is not open source. It has been released by BigScience under the Responsible AI License (RAIL). This does not impose restrictions on reuse, distribution, commercialization, and modifications, as long as you do not use it for one of the restricted use cases in Appendix A. No need to ask for permission in advance.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSpecifically: written Simplified Chinese\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAny serious attempt to train a large Dutch language model should actually start with compiling datasets with which you could properly evaluate such a model.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://goingdutch.ai/en/posts/bigscience-bloom/","summary":"Three volunteers. A couple of weeks of work. That\u0026rsquo;s what it took to add a language to BigScience BLOOM, the open multilingual language model with no fewer than 176 billion parameters that was released mid-2022. It aimed to become an open and multilingual alternative to GPT-3. In the end, 46 languages from all over the world made it into the dataset BLOOM was trained on. Even relatively small languages like Basque and Catalan managed to be included.","title":"Left behind: why the Dutch language is absent from Europe's foremost open language model"},{"content":"I can\u0026rsquo;t often publicly share details about the kind of projects we undertake at the Netherlands Forensic Institute with the help of AI, but at the recent EuroPython 2023 in Prague, I was able to discuss a case that unfolded a few years ago and on which the NFI had previously issued a press release: the Threat-to-Life project.\nPolice could read along with criminals In 2020, the police managed to read live messages from a provider of so-called cryptophones: modified phones that — for a substantial payment — were used for encrypted communication in the criminal circuit. It wasn\u0026rsquo;t the first time, nor the last, that the police managed to do this. It happens so frequently that there even exists a summary list of such operations against providers of cryptophones.\nIn practice, it turns out that some criminals feel extraordinarily safe using these cryptophones. They unabashedly transmit the most sensitive and incriminating messages without any obfuscation. Communication is key in the business world, apparently, no matter what kind of business you are in.\nDetecting threat-to-life messages Being able to read along is one thing, but when it comes to a large flow of messages, you want the police to be able to assess certain types of messages in a timely manner. For example, if discussions revolve around preparing for assaults, kidnappings, and assassinations, timely action must be taken to prevent these. So, something was needed: a threat-to-life detector.\nHerein lay the challenge: train a classification model that can find threat-to-life messages in large collections of non-threat-to-life messages from cryptophones. And although the task — classification — is not so innovative in itself, it is not trivial to get such a model off the ground. After all, you need to create a model that can handle the kind of language used in these messages: informal and riddled with street language and jargon. Quite different from the language you encounter when you scrape Wikipedia, for instance.\nIn addition, you must be able to gather enough training data — examples of the kind of messages you are looking for. And remember: these are relatively rare in the large stream of other messages. Kind of a chicken-and-egg problem actually.\nEuroPython 2023 You can see how we solved these problems in the live recording of my talk below. It was a relatively short talk for an audience of programmers, not necessarily data scientists. Therefore, I chose not to delve too deep into the details of the deep learning, and instead spent more time discussing the context of the entire story.\nBut that\u0026rsquo;s why I think it gives a nice peek behind the scenes: it shows what you encounter when deploying AI for a case like this.\nDozens of serious violent crimes prevented And the result? In the police press release from July 2020, the preliminary balance of the police operation was disclosed. It also shows what the police have been able to do with the threat-to-life signals that emerged from the investigation.\nBelow is the preliminary balance:\nOver 100 suspects arrested for very serious crimes Nearly 20 million euros in cash seized The seizure of 8000 kilograms of cocaine and over 1200 kilograms of crystal meth 19 synthetic drug labs dismantled Also, dozens of firearms were taken off the streets In the Netherlands alone, over 3000 signals that seemed life-threatening were processed in the past few months. By intervening timely, the police were able to prevent dozens of serious violent crimes, including impending kidnappings, extortions, assassinations, and tortures. Today, three years later, Europol is still keeping score of the entire operation. According to them, the counter had reached over 6,500 arrests and almost 900 million euros in cash and assets have been seized.\nAnd now? Cryptophones and decrypted messages were and are still very relevant in criminal cases. This is further illustrated by a recent news article from NOS on how the digital department of the NFI managed to crack hundreds of individual cryptophones.\nThis post was translated from the original Dutch with the help of GPT-4.\n","permalink":"https://goingdutch.ai/en/posts/europython-2023-ttl/","summary":"I can\u0026rsquo;t often publicly share details about the kind of projects we undertake at the Netherlands Forensic Institute with the help of AI, but at the recent EuroPython 2023 in Prague, I was able to discuss a case that unfolded a few years ago and on which the NFI had previously issued a press release: the Threat-to-Life project.\nPolice could read along with criminals In 2020, the police managed to read live messages from a provider of so-called cryptophones: modified phones that — for a substantial payment — were used for encrypted communication in the criminal circuit.","title":"My talk at EuroPython 2023: \"Threat to Life — Preventing Planned Murders with Python\""}]